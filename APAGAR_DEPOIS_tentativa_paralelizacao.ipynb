{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# import wandb\n",
    "import requests\n",
    "import progressbar\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_langs(translator, texts) -> list:\n",
    "    widgets = [\n",
    "        'Processing pages: ', progressbar.Percentage(),\n",
    "        ' ', progressbar.Bar(), ' ', progressbar.ETA(),\n",
    "        ' | ', progressbar.Counter(), ' Processed'\n",
    "    ]\n",
    "    pbar = progressbar.ProgressBar(maxval=len(titles), widgets=widgets).start()\n",
    "\n",
    "    detected_langs = []\n",
    "    for i, text in enumerate(texts):\n",
    "        detected_langs.append(translator.detect(text))\n",
    "        time.sleep(0.5)\n",
    "        pbar.update(i)\n",
    "\n",
    "    pbar.finish()\n",
    "\n",
    "    langs = [lang.lang for lang in detected_langs]\n",
    "    \n",
    "    return langs\n",
    "\n",
    "\n",
    "def translate_title(translator, titles, langs, dest=\"en\", src=\"pt\") -> list:\n",
    "    widgets = [\n",
    "        'Processing pages: ', progressbar.Percentage(),\n",
    "        ' ', progressbar.Bar(), ' ', progressbar.ETA(), \n",
    "        ' | ', progressbar.Counter(), ' Processed'\n",
    "    ]\n",
    "    pbar = progressbar.ProgressBar(maxval=len(titles), widgets=widgets).start()\n",
    "\n",
    "    titles_translated = []\n",
    "    for i, lang, title in zip(range(len(titles)), langs, titles):\n",
    "        if lang == \"en\":\n",
    "            titles_translated.append(title)\n",
    "        if lang == \"pt\":\n",
    "            titles_translated.append(translator.translate(title, dest=dest, src=src).text)\n",
    "            time.sleep(1)\n",
    "        pbar.update(i)\n",
    "\n",
    "    pbar.finish()\n",
    "    \n",
    "    return titles_translated\n",
    "\n",
    "\n",
    "def translate_keywords(translator, keywords, langs, dest=\"en\", src=\"pt\") -> list:\n",
    "    widgets = [\n",
    "        'Processing pages: ', progressbar.Percentage(),\n",
    "        ' ', progressbar.Bar(), ' ', progressbar.ETA(),\n",
    "        ' | ', progressbar.Counter(), ' Processed'\n",
    "    ]\n",
    "    pbar = progressbar.ProgressBar(maxval=len(keywords), widgets=widgets).start()\n",
    "\n",
    "    keywords_en = []\n",
    "    for i, lang, keywords_list in zip(range(len(keywords)), langs, keywords):\n",
    "        if lang == \"en\":\n",
    "            keywords_en.append(\", \".join(keywords_list))\n",
    "        if lang == \"pt\":\n",
    "            keywords_en.append(translator.translate(\", \".join(keywords_list), dest=dest, src=src).text)\n",
    "            time.sleep(2)\n",
    "        pbar.update(i)\n",
    "\n",
    "    pbar.finish()\n",
    "    \n",
    "    return keywords_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "PBAR_WIDGETS = [\"Processing pages: \", progressbar.Percentage(), \" \", progressbar.Bar(), \" \", progressbar.ETA()]\n",
    "LANDING_PAGE_URL = 'https://www.sba.org.br/open_journal_systems/index.php/cba'\n",
    "\n",
    "# Make a GET request to fetch the raw HTML content\n",
    "html_content = requests.get(LANDING_PAGE_URL).text\n",
    "landing_page = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "paper_pages = []\n",
    "obj_article = landing_page.body.find_all(\"div\", attrs={\"class\": \"obj_article_summary\"})\n",
    "for obj in obj_article:\n",
    "    paper_pages.append(obj.find(\"div\", attrs={\"class\": \"title\"}).a[\"href\"])\n",
    "    \n",
    "paper_pages = paper_pages[3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the parallelization of the ```URL GET``` requests\n",
    "\n",
    "### TL;DR\n",
    "Each request was made by a thread. The thread number was choosen to be 50.\n",
    "\n",
    "### Detailed\n",
    "This solution was heavily inspired by the code found on the link bellow\n",
    "\n",
    "https://www.shanelynn.ie/using-python-threading-for-multiple-results-queue/\n",
    "\n",
    "The ideia is basically to used threads to generate the requests (knowing that we don't have a real thread parallelism in python, but what seems to be one, given that all is executed concurrently). But then we encounter the basic problems of the parallelism universe: \n",
    "\n",
    "1. __Are we going to use a shared variable?__\n",
    "   \n",
    "    R: Yes, we are. But since the parallelism idea comes to the fact that there is a list of URLs where each request of one URL of it does not interfere on the request of another, the we can simply create a NEW array of the same size of that one, populate it with empty values (or strings or empty arrays) and then populate it on each request given that we pass an index of the URL on the first array to that new one. If it sounded confused, the idea is: the preprocessed data of the URL that occupies a given index on the first array will be at the same index of the second one. Just like that.\n",
    "\n",
    "2. __How can we choose an adequate number of threads to solve this problem without overflowing our RAM memory or the native python's thread creation limit (the python has a quite obscure limit number of threads that it can create)?__\n",
    "\n",
    "    R: This is purely arbitrary :) I just let the number used in the tutorial as it was. It seemed not so big and not so small.\n",
    "\n",
    "3. __How each thread will know the number of URLs it can process requests (since some threads can finish faster than others and so on)?__\n",
    "\n",
    "    R: For this we'll use the multithreading data structure ```Queue()``` from the ```queue``` library. We populate it if pairs (index, url_correspondent_that_index) of the original array. And on each thread, we simply verify if the queue is not empty. If that's the case, we can pop an element of it (that will be reflected to the other threads) and make a request with it. Since we have also the index of the original element on the original array, it is as described on item __1)__ above.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "from queue import Queue\n",
    "from threading import Thread\n",
    "\n",
    "# number of URL pages\n",
    "n_pages = len(paper_pages)\n",
    "# a queue for threading with \"infinite\" size\n",
    "url_queue = Queue(maxsize=0)\n",
    "# NUM_THREADS is a number in [1, 50]\n",
    "NUM_THREADS = min(50, n_pages)\n",
    "\n",
    "# the shared variable must be outside the threads' scope, then we instantiate it here.\n",
    "# each one is a array of arrays or strings with n_pages elements.\n",
    "results = {\n",
    "    'titles':       ['']    * n_pages,\n",
    "    'authors':      [ [] ]  * n_pages,\n",
    "    'affiliations': [ [] ]  * n_pages,\n",
    "    'dois':         [ [] ]  * n_pages,\n",
    "    'keywords':     [ [] ]  * n_pages,\n",
    "    'abstracts':    [ [] ]  * n_pages\n",
    "}\n",
    "\n",
    "# starting the Queue threading structure and populating it with indexes and the \n",
    "# URLS that will be used by the thread workers.\n",
    "for i, url in enumerate(paper_pages):\n",
    "    url_pair = (i, url)\n",
    "    url_queue.put(url_pair)\n",
    "\n",
    "\n",
    "def scrap_url(queue: Queue, results: dict, pbar: progressbar.bar.ProgressBar) -> None:\n",
    "    while not queue.empty():\n",
    "        INDEX, URL = queue.get()\n",
    "\n",
    "        try:\n",
    "            # flag that indicates if the request was well succeeded.\n",
    "            proceed = False\n",
    "            while not proceed:\n",
    "                # this block is what guarantee that failed requests execute again\n",
    "                try:\n",
    "                    html_content = requests.get(URL, timeout=15).text\n",
    "                    page = BeautifulSoup(html_content, \"html.parser\")\n",
    "                except Exception as err:\n",
    "                    # all exceptions here are indiscriminatly treated the same\n",
    "                    # way: ignoring them, since the only thing possible to do\n",
    "                    # is to try the GET request again.\n",
    "                    pass\n",
    "                else:\n",
    "                    if page.body is not None:\n",
    "                        proceed = True\n",
    "\n",
    "            # get paper title\n",
    "            current_title = page.find(\"h1\", class_=\"page_title\").text.strip()\n",
    "            results['titles'][INDEX] = current_title\n",
    "\n",
    "            # get author names\n",
    "            author_list = page.body.find(\"article\", attrs={\"class\": \"obj_article_details\"}).\\\n",
    "            find_all(\"span\", attrs={\"class\": \"name\"})\n",
    "\n",
    "            current_authors_list = []\n",
    "            for author_span in author_list:\n",
    "                current_authors_list.append(author_span.text.strip())\n",
    "\n",
    "            results['authors'][INDEX] = current_authors_list\n",
    "\n",
    "            # get author affiliations\n",
    "            affiliations_list = page.body.find(\"article\", attrs={\"class\": \"obj_article_details\"}).\\\n",
    "            find_all(\"span\", attrs={\"class\": \"affiliation\"})\n",
    "\n",
    "            current_affiliations_list = []\n",
    "            for affiliation_span in affiliations_list:\n",
    "                current_affiliations_list.append(affiliation_span.text.strip())\n",
    "\n",
    "            results['affiliations'][INDEX] = current_affiliations_list\n",
    "\n",
    "            # get doi\n",
    "            current_doi = page.body.find(\"article\", attrs={\"class\": \"obj_article_details\"}).\\\n",
    "            find(\"div\", class_=\"item doi\").a.string.strip()\n",
    "            results['dois'][INDEX] = [current_doi]\n",
    "\n",
    "            # get keywords\n",
    "            current_keywords = page.body.find(\"article\", attrs={\"class\": \"obj_article_details\"}).\\\n",
    "            find(\"div\", class_=\"item keywords\").find(\"span\", attrs={\"class\": \"value\"}).text.split(\",\")\n",
    "            current_keywords = [keyword.strip() for keyword in current_keywords]\n",
    "            results['keywords'][INDEX] = current_keywords\n",
    "\n",
    "            # get abstract\n",
    "            current_abstract = [page.find(\"div\", class_=\"item abstract\").p.text.strip()]\n",
    "            results['abstracts'][INDEX] = current_abstract\n",
    "        # just in case of any error at the try, just to keep the downloading the\n",
    "        # pipeline safe.\n",
    "        except Exception as err:\n",
    "            logger.warning(f'{err} (at `results` on index {INDEX}) error when incorporating elements of {URL}')\n",
    "\n",
    "            # if an error occurs, then we have put the same pair (INDEX, URL)\n",
    "            # to the queue otherwise, it can have an incomplete set of values\n",
    "            url_pair = (INDEX, URL)\n",
    "            url_queue.put(url_pair)\n",
    "        else:\n",
    "            pbar.update(n_pages - queue.qsize())\n",
    "\n",
    "        # sinalizes to the queue that the job is done (important to the .join() )\n",
    "        queue.task_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pages: 100% |#######################################| Time:  0:01:02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tasks concluded.\n"
     ]
    }
   ],
   "source": [
    "# defining the progress bar\n",
    "pbar = progressbar.ProgressBar(maxval=n_pages, widgets=PBAR_WIDGETS).start()\n",
    "\n",
    "for i in range(NUM_THREADS):\n",
    "    # starting the threads with the queue and the shared variable of results\n",
    "    worker = Thread(target=scrap_url, args=(url_queue, results, pbar))\n",
    "    worker.setDaemon(True)    # setting threads as \"daemon\" allows main program to \n",
    "                              # exit eventually even if these dont finish \n",
    "                              # correctly.\n",
    "    worker.start()\n",
    "\n",
    "url_queue.join()\n",
    "pbar.finish()\n",
    "\n",
    "print('All tasks concluded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segmenting the results lists\n",
    "titles = results['titles']\n",
    "authors = results['authors']\n",
    "affiliations = results['affiliations']\n",
    "dois = results['dois']\n",
    "keywords = results['keywords']\n",
    "abstracts = results['abstracts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('titles.json', 'wt') as save_path:\n",
    "    json.dump(titles, save_path, indent=2)\n",
    "\n",
    "with open('titles.json', 'rt') as read_path:\n",
    "    AIAI = json.load(read_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"titles.json\", \"r\") as f:\n",
    "#     titles = json.load(f)\n",
    "\n",
    "# with open(\"authors.json\", \"r\") as f:\n",
    "#     authors = json.load(f)\n",
    "    \n",
    "# with open(\"affiliations.json\", \"r\") as f:\n",
    "#     affiliations = json.load(f)\n",
    "    \n",
    "# with open(\"dois.json\", \"r\") as f:\n",
    "#     dois = json.load(f)\n",
    "    \n",
    "# with open(\"keywords.json\", \"r\") as f:\n",
    "#     keywords = json.load(f)\n",
    "    \n",
    "# with open(\"abstracts.json\", \"r\") as f:\n",
    "#     abstracts = json.load(f)\n",
    "    \n",
    "# with open(\"langs.json\", \"r\") as f:\n",
    "#     langs = json.load(f)\n",
    "    \n",
    "# with open(\"titles_en.json\", \"r\") as f:\n",
    "#     titles_en = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pages: 100% |#######################| Time:  0:10:29 | 777 Processed\n",
      "Processing pages: 100% |#######################| Time:  0:13:30 | 777 Processed\n",
      "Processing pages: 100% |#######################| Time:  0:24:30 | 777 Processed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "langs = detect_langs(translator, titles)\n",
    "titles_en = translate_title(translator, titles, langs)\n",
    "keywords_en = translate_keywords(translator, keywords, langs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('download_data')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1a2134588e63c6df46b2e2e4b93ab8be2947a4b7323806956d805eda4708b858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
